{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0195ec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import scipy as scp\n",
    "import sklearn as skl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdb8dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-python==4.2.0.34\n",
    "#!pip install imgaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5de09ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug.augmenters as iaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33f7758f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ccb202c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.image.AxesImage at 0x200af1fbf70>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "plt.imshow(x_train[0])\n",
    "#x_train = x_train.reshape(-1, 784).astype(\"float32\") / 255.0\n",
    "#x_test = x_test.reshape(-1, 784).astype(\"float32\") / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "(28, 28, 3)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([x_test[0].reshape(28,28,1),x_test[0].reshape(28,28,1),x_test[0].reshape(28,28,1)],axis = 2).shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "aug = iaa.Grayscale(alpha=(0.0, 1.0))\n",
    "def def_augment(n=3, m=7):\n",
    "    #\n",
    "    rand_aug = iaa.RandAugment(n=n, m=m)\n",
    "    #     @tf.numpy_function\n",
    "    def augment(images):\n",
    "        # Input to `augment()` is a TensorFlow tensor which\n",
    "        # is not supported by `imgaug`. This is why we first\n",
    "        # convert it to its `numpy` variant.\n",
    "        print(images.shape)\n",
    "        images = aug(images=images)\n",
    "        return rand_aug(images=images).reshape(-1,784)\n",
    "    return augment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#aug_bis = def_augment()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "#x_color = x_test.reshape(-1,28,28,1)\n",
    "#x_color = np.concatenate([x_color,x_color,x_color],axis = 3)\n",
    "#a = aug_bis(images = x_color)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08aa61c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100 # the number of repetition * batch_size must be <= to the dataset size\n",
    "nb_classes = 10\n",
    "#def data_loader_custom(X,augmentation_1 = lambda x : x ,augmentation_2 = lambda x : x, nb_classes = nb_classes):\n",
    "def data_loader_custom(X,augmentation_1 = def_augment(n=2, m=21) ,augmentation_2 = def_augment(n=3, m=12), nb_classes = nb_classes):\n",
    "\n",
    "    def configure_for_performance(ds):\n",
    "        ds = ds.shuffle(buffer_size=2*batch_size)\n",
    "        ds = ds.batch(batch_size=batch_size)\n",
    "        ds = ds.repeat()\n",
    "        ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "        return ds\n",
    "    \n",
    "    X_color = X.reshape(-1,28,28,1)\n",
    "    X_color = np.concatenate([X_color,X_color,X_color],axis = 3)\n",
    "    print(\"FIRST PART\")\n",
    "    X_aug1 = augmentation_1(X_color)\n",
    "    print(\"SECOND PART\")\n",
    "    X_aug2 = augmentation_2(X_color)\n",
    "    #print(\"THIRD PART\")\n",
    "    #y = tf.constant(np.random.randint(0,2,size=(len(X_aug1),nb_classes)))\n",
    "\n",
    "    #fake_labels = tf.data.Dataset.from_tensor_slices(y)\n",
    "    ds = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(X_aug1), tf.data.Dataset.from_tensor_slices(X_aug2)))\n",
    "    #X = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(X_aug1), tf.data.Dataset.from_tensor_slices(X_aug2)))\n",
    "    #print(\"JUNCTION\")\n",
    "    #ds = tf.data.Dataset.zip((X,fake_labels))\n",
    "    ds = configure_for_performance(ds)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c194fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST PART\n",
      "(60000, 28, 28, 3)\n",
      "SECOND PART\n",
      "(60000, 28, 28, 3)\n",
      "FIRST PART\n",
      "(10000, 28, 28, 3)\n",
      "SECOND PART\n",
      "(10000, 28, 28, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train = data_loader_custom(x_train)\n",
    "X_test =  data_loader_custom(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 784), dtype=tf.uint8, name=None), TensorSpec(shape=(None, 784), dtype=tf.uint8, name=None))>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d47d1a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape[0]%batch_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60755db0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[0]%batch_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88ef4d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\n",
    "\n",
    "def tf_unif_prior_crossentropy(pred1,pred2):\n",
    "        N = batch_size\n",
    "        C = nb_classes\n",
    "        t_c = 0.1\n",
    "        t_r = 0.1\n",
    "        a = tf.nn.softmax(pred2/t_c, axis=0)\n",
    "        b = tf.linalg.norm(a, ord=1,axis=1)\n",
    "        try :\n",
    "            c = a/tf.reshape(b,(N,-1))\n",
    "        except :\n",
    "            try :\n",
    "                c = a/tf.reshape(b,(x_train.shape[0]%batch_size,-1))\n",
    "            except :\n",
    "                c = a/tf.reshape(b,(x_test.shape[0]%batch_size,-1))\n",
    "                \n",
    "\n",
    "        part11 = c\n",
    "        d = tf.nn.softmax(pred1/t_r, axis=1)\n",
    "        part21 = tf.reshape(tf.math.log(N/C *d/tf.linalg.norm(d, ord=1, axis=0)), (-1,C))\n",
    "        loss_1 = -tf.math.reduce_sum( part11 * part21 , axis=-1)\n",
    "\n",
    "\n",
    "        a = tf.nn.softmax(pred1/t_c)\n",
    "        b = tf.linalg.norm(a, ord=1,axis=1)\n",
    "        c = a/tf.reshape(b,(N,-1))\n",
    "        part12 = c\n",
    "        d = tf.nn.softmax(pred2/t_r, axis=1)\n",
    "        part22 = tf.reshape(tf.math.log(N/C *d/tf.linalg.norm(d, ord=1, axis=0)),(-1,C))\n",
    "        loss_2 = -tf.math.reduce_sum( part12*part22 , axis=-1)\n",
    "\n",
    "        return tf.math.reduce_mean(1/2*(loss_1+loss_2))\n",
    "\n",
    "class custom_fit(tf.keras.Model):\n",
    "    \n",
    "    \n",
    "    \n",
    "    def train_step(self, data):\n",
    "        input_data = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self(input_data, training=True) # forward pass \n",
    "            pred1 = outputs[0]\n",
    "            pred2 = outputs[1]\n",
    "            total_loss = tf_unif_prior_crossentropy(pred1,pred2)\n",
    "        gradients = tape.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        avg_loss.update_state(total_loss)\n",
    "        return {\"loss\": avg_loss.result()}\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        # If you don't implement this property, you have to call\n",
    "        # `reset_states()` yourself at the time of your choosing.\n",
    "        return [avg_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58e58699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_aux(dropout = 0.5):\n",
    "    \n",
    "    input_layer = tf.keras.layers.Input(shape=(28*28), dtype=tf.float32)\n",
    "\n",
    "    first_layer = tf.keras.layers.Dense(512, activation = \"selu\")(input_layer)\n",
    "#     first_layer = tf.keras.layers.Dropout(dropout)(first_layer)\n",
    "    first_layer = tf.keras.layers.BatchNormalization()(first_layer)\n",
    "    first_layer = tf.keras.layers.Dense(256, activation = \"selu\")(first_layer)\n",
    "#     first_layer = tf.keras.layers.Dropout(dropout)(first_layer)\n",
    "    first_layer = tf.keras.layers.BatchNormalization()(first_layer)\n",
    "    first_layer = tf.keras.layers.Dense(128, activation = \"selu\")(first_layer)\n",
    "#     first_layer = tf.keras.layers.Dropout(dropout)(first_layer)\n",
    "    first_layer = tf.keras.layers.BatchNormalization()(first_layer)\n",
    "\n",
    "    output = tf.keras.layers.Dense(nb_classes,activation = None, dtype='float32')(first_layer)\n",
    "#     output = tf.keras.layers.Dropout(dropout)(output)\n",
    "    output = tf.keras.layers.BatchNormalization()(output)\n",
    "    output = tf.keras.layers.Activation(\"softmax\")(output)\n",
    "    model = tf.keras.models.Model(inputs=input_layer, outputs= output)\n",
    "    return model\n",
    "\n",
    "def create_model():\n",
    "    model_shared = create_model_aux()\n",
    "    input_x1 = tf.keras.layers.Input(shape=(28*28), dtype=tf.float32)\n",
    "    input_x2 = tf.keras.layers.Input(shape=(28*28), dtype=tf.float32)\n",
    "    output_x1 = model_shared(input_x1)\n",
    "    output_x2 = model_shared(input_x2)\n",
    "#     output = tf.keras.layers.Average()([output_x1, output_x2])\n",
    "#     model = tf.keras.models.Model(inputs=[input_x1,input_x2], outputs = output)\n",
    "    model = custom_fit(inputs=[input_x1,input_x2], outputs = [output_x1,output_x2])\n",
    "    model.compile(loss=tf_unif_prior_crossentropy,metrics=[],optimizer = \"Adam\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0351e6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f72355f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_min = 1e-5\n",
    "lr_max = 1e-2\n",
    "import os\n",
    "def my_cosine_decay(epoch,\n",
    "                    epoch_max=5,\n",
    "                    LR_START = 0.0001/2,\n",
    "                    LR_MAX =  lr_max,\n",
    "                    LR_MIN = lr_min,\n",
    "                    LR_RAMPUP_EPOCHS = 3,\n",
    "                    LR_SUSTAIN_EPOCHS = 0,\n",
    "                    LR_ALPHA_DECAY = .7\n",
    "                   ):\n",
    "    if epoch < LR_RAMPUP_EPOCHS:\n",
    "        learning_rate = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
    "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
    "        learning_rate = LR_MAX\n",
    "    else:\n",
    "        cosine_decay=max(0.5*(1+np.cos(np.pi*((epoch-LR_RAMPUP_EPOCHS)/epoch_max))),LR_MIN)\n",
    "        decayed=(1-LR_ALPHA_DECAY)*cosine_decay+LR_ALPHA_DECAY\n",
    "        learning_rate=LR_MAX*decayed\n",
    "    return learning_rate\n",
    "\n",
    "def lr_decay(epoch,\n",
    "            LR_START = lr_max/2,\n",
    "            LR_MAX =  lr_max,\n",
    "            LR_MIN = lr_min,\n",
    "            LR_RAMPUP_EPOCHS = 3,\n",
    "            LR_SUSTAIN_EPOCHS = 0,\n",
    "            LR_ALPHA_DECAY = .05\n",
    "                   ):\n",
    "    learning_rate=max(LR_MAX*np.exp(-LR_ALPHA_DECAY*epoch), LR_MIN)\n",
    "    return learning_rate\n",
    "   \n",
    "\n",
    "def my_clr(epoch,\n",
    "           epoch_cycle = 4,\n",
    "           LR_MIN = lr_min,\n",
    "           LR_MAX = lr_max,\n",
    "           LR_ALPHA_DECAY = .9):\n",
    "    nb_cycle = epoch // epoch_cycle\n",
    "    k=0\n",
    "    while k < nb_cycle :\n",
    "        LR_MAX = (LR_MAX+ LR_MIN)*LR_ALPHA_DECAY\n",
    "        k += 1\n",
    "    if epoch % epoch_cycle < epoch_cycle//2 :\n",
    "        learning_rate = (LR_MAX - LR_MIN) / (epoch_cycle//2) * (epoch % (epoch_cycle//2)) + LR_MIN\n",
    "    else :\n",
    "        learning_rate = -(LR_MAX - LR_MIN) / (epoch_cycle//2) * (epoch % (epoch_cycle//2)) + LR_MAX\n",
    "    return learning_rate\n",
    "    \n",
    "lrfn=lambda epoch : my_clr(\n",
    "       epoch,\n",
    "        LR_MIN = lr_min,\n",
    "        LR_MAX = lr_max\n",
    "    )\n",
    "\n",
    "checkpoint_filepath = r\"C:\\Users\\bapti\\KTH\\mnist\\checkpoint\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_filepath)\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='loss',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)\n",
    "early_stopper = tf.keras.callbacks.EarlyStopping(monitor='loss',patience=16,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47d4e21a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 1/1000\n",
      "600/600 [==============================] - 15s 22ms/step - loss: 2.5059 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.005005.\n",
      "Epoch 2/1000\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 1.9342 - lr: 0.0050\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.01.\n",
      "Epoch 3/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.6831 - lr: 0.0100\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.005005.\n",
      "Epoch 4/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.5393 - lr: 0.0050\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 5/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.5032 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0045095.\n",
      "Epoch 6/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 1.5127 - lr: 0.0045\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.009009.\n",
      "Epoch 7/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.5335 - lr: 0.0090\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0045095.\n",
      "Epoch 8/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.4006 - lr: 0.0045\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 9/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.3671 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.00406355.\n",
      "Epoch 10/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 1.3632 - lr: 0.0041\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.0081171.\n",
      "Epoch 11/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 1.4236 - lr: 0.0081\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.00406355.\n",
      "Epoch 12/1000\n",
      "600/600 [==============================] - 17s 28ms/step - loss: 1.3510 - lr: 0.0041\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 13/1000\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 1.3240 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.0036621950000000005.\n",
      "Epoch 14/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 1.3170 - lr: 0.0037\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.0073143900000000005.\n",
      "Epoch 15/1000\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 1.3750 - lr: 0.0073\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.003662195.\n",
      "Epoch 16/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.3106 - lr: 0.0037\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 17/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 1.2929 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.0033009755000000004.\n",
      "Epoch 18/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.2883 - lr: 0.0033\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.006591951.\n",
      "Epoch 19/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 1.3258 - lr: 0.0066\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.0033009755.\n",
      "Epoch 20/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.2896 - lr: 0.0033\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 21/1000\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 1.2662 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.00297587795.\n",
      "Epoch 22/1000\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 1.2585 - lr: 0.0030\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.0059417559.\n",
      "Epoch 23/1000\n",
      "600/600 [==============================] - 14s 23ms/step - loss: 1.3069 - lr: 0.0059\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.0029758779499999997.\n",
      "Epoch 24/1000\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 1.2747 - lr: 0.0030\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 25/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.2539 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.002683290155.\n",
      "Epoch 26/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.2428 - lr: 0.0027\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.005356580309999999.\n",
      "Epoch 27/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.2965 - lr: 0.0054\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.0026832901549999994.\n",
      "Epoch 28/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 1.2506 - lr: 0.0027\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 29/1000\n",
      "600/600 [==============================] - 14s 24ms/step - loss: 1.2333 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.0024199611394999996.\n",
      "Epoch 30/1000\n",
      "600/600 [==============================] - 14s 24ms/step - loss: 1.2317 - lr: 0.0024\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 0.004829922278999999.\n",
      "Epoch 31/1000\n",
      "600/600 [==============================] - 14s 23ms/step - loss: 1.2588 - lr: 0.0048\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 0.002419961139499999.\n",
      "Epoch 32/1000\n",
      "600/600 [==============================] - 15s 25ms/step - loss: 1.2408 - lr: 0.0024\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 33/1000\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 1.2271 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 0.0021829650255499994.\n",
      "Epoch 34/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.2099 - lr: 0.0022\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 0.004355930051099998.\n",
      "Epoch 35/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.2468 - lr: 0.0044\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 0.002182965025549999.\n",
      "Epoch 36/1000\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 1.2279 - lr: 0.0022\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 37/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.2047 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 0.0019696685229949992.\n",
      "Epoch 38/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.2000 - lr: 0.0020\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 0.003929337045989998.\n",
      "Epoch 39/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 1.2375 - lr: 0.0039\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 0.001969668522994999.\n",
      "Epoch 40/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.2099 - lr: 0.0020\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 41/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.2005 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 0.001777701670695499.\n",
      "Epoch 42/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.1908 - lr: 0.0018\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 0.003545403341390998.\n",
      "Epoch 43/1000\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 1.2169 - lr: 0.0035\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 0.001777701670695499.\n",
      "Epoch 44/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.2026 - lr: 0.0018\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 45/1000\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 1.1937 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 0.001604931503625949.\n",
      "Epoch 46/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.1740 - lr: 0.0016\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 0.003199863007251898.\n",
      "Epoch 47/1000\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 1.2055 - lr: 0.0032\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 0.001604931503625949.\n",
      "Epoch 48/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 1.1924 - lr: 0.0016\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 49/1000\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 1.1779 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 0.0014494383532633543.\n",
      "Epoch 50/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.1680 - lr: 0.0014\n",
      "\n",
      "Epoch 51: LearningRateScheduler setting learning rate to 0.0028888767065267085.\n",
      "Epoch 51/1000\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 1.1978 - lr: 0.0029\n",
      "\n",
      "Epoch 52: LearningRateScheduler setting learning rate to 0.0014494383532633543.\n",
      "Epoch 52/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.1781 - lr: 0.0014\n",
      "\n",
      "Epoch 53: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 53/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.1649 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 54: LearningRateScheduler setting learning rate to 0.0013094945179370188.\n",
      "Epoch 54/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.1606 - lr: 0.0013\n",
      "\n",
      "Epoch 55: LearningRateScheduler setting learning rate to 0.0026089890358740376.\n",
      "Epoch 55/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.1796 - lr: 0.0026\n",
      "\n",
      "Epoch 56: LearningRateScheduler setting learning rate to 0.0013094945179370188.\n",
      "Epoch 56/1000\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 1.1738 - lr: 0.0013\n",
      "\n",
      "Epoch 57: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 57/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.1635 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 58: LearningRateScheduler setting learning rate to 0.001183545066143317.\n",
      "Epoch 58/1000\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 1.1468 - lr: 0.0012\n",
      "\n",
      "Epoch 59: LearningRateScheduler setting learning rate to 0.002357090132286634.\n",
      "Epoch 59/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 1.1710 - lr: 0.0024\n",
      "\n",
      "Epoch 60: LearningRateScheduler setting learning rate to 0.001183545066143317.\n",
      "Epoch 60/1000\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 1.1652 - lr: 0.0012\n",
      "\n",
      "Epoch 61: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 61/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 1.1500 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 62: LearningRateScheduler setting learning rate to 0.0010701905595289854.\n",
      "Epoch 62/1000\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 1.1449 - lr: 0.0011\n",
      "\n",
      "Epoch 63: LearningRateScheduler setting learning rate to 0.0021303811190579707.\n",
      "Epoch 63/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.1652 - lr: 0.0021\n",
      "\n",
      "Epoch 64: LearningRateScheduler setting learning rate to 0.0010701905595289854.\n",
      "Epoch 64/1000\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 1.1533 - lr: 0.0011\n",
      "\n",
      "Epoch 65: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 65/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 1.1471 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 66: LearningRateScheduler setting learning rate to 0.0009681715035760868.\n",
      "Epoch 66/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.1393 - lr: 9.6817e-04\n",
      "\n",
      "Epoch 67: LearningRateScheduler setting learning rate to 0.0019263430071521737.\n",
      "Epoch 67/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 1.1511 - lr: 0.0019\n",
      "\n",
      "Epoch 68: LearningRateScheduler setting learning rate to 0.0009681715035760868.\n",
      "Epoch 68/1000\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 1.1476 - lr: 9.6817e-04\n",
      "\n",
      "Epoch 69: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 69/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 1.1418 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 70: LearningRateScheduler setting learning rate to 0.0008763543532184782.\n",
      "Epoch 70/1000\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 1.1252 - lr: 8.7635e-04\n",
      "\n",
      "Epoch 71: LearningRateScheduler setting learning rate to 0.0017427087064369564.\n",
      "Epoch 71/1000\n",
      "600/600 [==============================] - 14s 24ms/step - loss: 1.1464 - lr: 0.0017\n",
      "\n",
      "Epoch 72: LearningRateScheduler setting learning rate to 0.0008763543532184782.\n",
      "Epoch 72/1000\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 1.1442 - lr: 8.7635e-04\n",
      "\n",
      "Epoch 73: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 73/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.1297 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 74: LearningRateScheduler setting learning rate to 0.0007937189178966305.\n",
      "Epoch 74/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.1237 - lr: 7.9372e-04\n",
      "\n",
      "Epoch 75: LearningRateScheduler setting learning rate to 0.0015774378357932609.\n",
      "Epoch 75/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.1438 - lr: 0.0016\n",
      "\n",
      "Epoch 76: LearningRateScheduler setting learning rate to 0.0007937189178966305.\n",
      "Epoch 76/1000\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 1.1299 - lr: 7.9372e-04\n",
      "\n",
      "Epoch 77: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 77/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.1282 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 78: LearningRateScheduler setting learning rate to 0.0007193470261069675.\n",
      "Epoch 78/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.1176 - lr: 7.1935e-04\n",
      "\n",
      "Epoch 79: LearningRateScheduler setting learning rate to 0.0014286940522139349.\n",
      "Epoch 79/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.1283 - lr: 0.0014\n",
      "\n",
      "Epoch 80: LearningRateScheduler setting learning rate to 0.0007193470261069675.\n",
      "Epoch 80/1000\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 1.1296 - lr: 7.1935e-04\n",
      "\n",
      "Epoch 81: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 81/1000\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 1.1243 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 82: LearningRateScheduler setting learning rate to 0.0006524123234962708.\n",
      "Epoch 82/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 1.1113 - lr: 6.5241e-04\n",
      "\n",
      "Epoch 83: LearningRateScheduler setting learning rate to 0.0012948246469925415.\n",
      "Epoch 83/1000\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 1.1261 - lr: 0.0013\n",
      "\n",
      "Epoch 84: LearningRateScheduler setting learning rate to 0.0006524123234962708.\n",
      "Epoch 84/1000\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 1.1263 - lr: 6.5241e-04\n",
      "\n",
      "Epoch 85: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 85/1000\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 1.1099 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 86: LearningRateScheduler setting learning rate to 0.0005921710911466438.\n",
      "Epoch 86/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.1042 - lr: 5.9217e-04\n",
      "\n",
      "Epoch 87: LearningRateScheduler setting learning rate to 0.0011743421822932875.\n",
      "Epoch 87/1000\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 1.1203 - lr: 0.0012\n",
      "\n",
      "Epoch 88: LearningRateScheduler setting learning rate to 0.0005921710911466438.\n",
      "Epoch 88/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.1139 - lr: 5.9217e-04\n",
      "\n",
      "Epoch 89: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 89/1000\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 1.1121 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 90: LearningRateScheduler setting learning rate to 0.0005379539820319795.\n",
      "Epoch 90/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.1030 - lr: 5.3795e-04\n",
      "\n",
      "Epoch 91: LearningRateScheduler setting learning rate to 0.001065907964063959.\n",
      "Epoch 91/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.1074 - lr: 0.0011\n",
      "\n",
      "Epoch 92: LearningRateScheduler setting learning rate to 0.0005379539820319795.\n",
      "Epoch 92/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.1105 - lr: 5.3795e-04\n",
      "\n",
      "Epoch 93: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 93/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.1091 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 94: LearningRateScheduler setting learning rate to 0.0004891585838287815.\n",
      "Epoch 94/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0935 - lr: 4.8916e-04\n",
      "\n",
      "Epoch 95: LearningRateScheduler setting learning rate to 0.0009683171676575631.\n",
      "Epoch 95/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.1090 - lr: 9.6832e-04\n",
      "\n",
      "Epoch 96: LearningRateScheduler setting learning rate to 0.0004891585838287815.\n",
      "Epoch 96/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.1107 - lr: 4.8916e-04\n",
      "\n",
      "Epoch 97: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 97/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0954 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 98: LearningRateScheduler setting learning rate to 0.00044524272544590337.\n",
      "Epoch 98/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0949 - lr: 4.4524e-04\n",
      "\n",
      "Epoch 99: LearningRateScheduler setting learning rate to 0.0008804854508918067.\n",
      "Epoch 99/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.1077 - lr: 8.8049e-04\n",
      "\n",
      "Epoch 100: LearningRateScheduler setting learning rate to 0.00044524272544590337.\n",
      "Epoch 100/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0998 - lr: 4.4524e-04\n",
      "\n",
      "Epoch 101: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 101/1000\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 1.0981 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 102: LearningRateScheduler setting learning rate to 0.00040571845290131303.\n",
      "Epoch 102/1000\n",
      "600/600 [==============================] - 14s 23ms/step - loss: 1.0931 - lr: 4.0572e-04\n",
      "\n",
      "Epoch 103: LearningRateScheduler setting learning rate to 0.000801436905802626.\n",
      "Epoch 103/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0929 - lr: 8.0144e-04\n",
      "\n",
      "Epoch 104: LearningRateScheduler setting learning rate to 0.00040571845290131303.\n",
      "Epoch 104/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 1.1004 - lr: 4.0572e-04\n",
      "\n",
      "Epoch 105: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 105/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 1.0976 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 106: LearningRateScheduler setting learning rate to 0.00037014660761118176.\n",
      "Epoch 106/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0804 - lr: 3.7015e-04\n",
      "\n",
      "Epoch 107: LearningRateScheduler setting learning rate to 0.0007302932152223635.\n",
      "Epoch 107/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0973 - lr: 7.3029e-04\n",
      "\n",
      "Epoch 108: LearningRateScheduler setting learning rate to 0.00037014660761118176.\n",
      "Epoch 108/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0967 - lr: 3.7015e-04\n",
      "\n",
      "Epoch 109: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 109/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0848 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 110: LearningRateScheduler setting learning rate to 0.0003381319468500636.\n",
      "Epoch 110/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0829 - lr: 3.3813e-04\n",
      "\n",
      "Epoch 111: LearningRateScheduler setting learning rate to 0.0006662638937001272.\n",
      "Epoch 111/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 1.0969 - lr: 6.6626e-04\n",
      "\n",
      "Epoch 112: LearningRateScheduler setting learning rate to 0.0003381319468500636.\n",
      "Epoch 112/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 1.0865 - lr: 3.3813e-04\n",
      "\n",
      "Epoch 113: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 113/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 1.0900 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 114: LearningRateScheduler setting learning rate to 0.0003093187521650573.\n",
      "Epoch 114/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 1.0841 - lr: 3.0932e-04\n",
      "\n",
      "Epoch 115: LearningRateScheduler setting learning rate to 0.0006086375043301145.\n",
      "Epoch 115/1000\n",
      "600/600 [==============================] - 14s 24ms/step - loss: 1.0810 - lr: 6.0864e-04\n",
      "\n",
      "Epoch 116: LearningRateScheduler setting learning rate to 0.0003093187521650573.\n",
      "Epoch 116/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 1.0885 - lr: 3.0932e-04\n",
      "\n",
      "Epoch 117: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 117/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0846 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 118: LearningRateScheduler setting learning rate to 0.0002833868769485516.\n",
      "Epoch 118/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 1.0701 - lr: 2.8339e-04\n",
      "\n",
      "Epoch 119: LearningRateScheduler setting learning rate to 0.0005567737538971032.\n",
      "Epoch 119/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0723 - lr: 5.5677e-04\n",
      "\n",
      "Epoch 120: LearningRateScheduler setting learning rate to 0.0002833868769485516.\n",
      "Epoch 120/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0733 - lr: 2.8339e-04\n",
      "\n",
      "Epoch 121: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 121/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0586 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 122: LearningRateScheduler setting learning rate to 0.00026004818925369646.\n",
      "Epoch 122/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 1.0627 - lr: 2.6005e-04\n",
      "\n",
      "Epoch 123: LearningRateScheduler setting learning rate to 0.0005100963785073929.\n",
      "Epoch 123/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0682 - lr: 5.1010e-04\n",
      "\n",
      "Epoch 124: LearningRateScheduler setting learning rate to 0.00026004818925369646.\n",
      "Epoch 124/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 1.0438 - lr: 2.6005e-04\n",
      "\n",
      "Epoch 125: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 125/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0436 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 126: LearningRateScheduler setting learning rate to 0.0002390433703283268.\n",
      "Epoch 126/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 1.0381 - lr: 2.3904e-04\n",
      "\n",
      "Epoch 127: LearningRateScheduler setting learning rate to 0.0004680867406566536.\n",
      "Epoch 127/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0350 - lr: 4.6809e-04\n",
      "\n",
      "Epoch 128: LearningRateScheduler setting learning rate to 0.00023904337032832682.\n",
      "Epoch 128/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0386 - lr: 2.3904e-04\n",
      "\n",
      "Epoch 129: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 129/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0319 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 130: LearningRateScheduler setting learning rate to 0.00022013903329549413.\n",
      "Epoch 130/1000\n",
      "600/600 [==============================] - 14s 23ms/step - loss: 1.0176 - lr: 2.2014e-04\n",
      "\n",
      "Epoch 131: LearningRateScheduler setting learning rate to 0.0004302780665909883.\n",
      "Epoch 131/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0294 - lr: 4.3028e-04\n",
      "\n",
      "Epoch 132: LearningRateScheduler setting learning rate to 0.00022013903329549416.\n",
      "Epoch 132/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 1.0317 - lr: 2.2014e-04\n",
      "\n",
      "Epoch 133: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 133/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0192 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 134: LearningRateScheduler setting learning rate to 0.00020312512996594474.\n",
      "Epoch 134/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 1.0167 - lr: 2.0313e-04\n",
      "\n",
      "Epoch 135: LearningRateScheduler setting learning rate to 0.0003962502599318895.\n",
      "Epoch 135/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 1.0228 - lr: 3.9625e-04\n",
      "\n",
      "Epoch 136: LearningRateScheduler setting learning rate to 0.00020312512996594476.\n",
      "Epoch 136/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0196 - lr: 2.0313e-04\n",
      "\n",
      "Epoch 137: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 137/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0155 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 138: LearningRateScheduler setting learning rate to 0.00018781261696935027.\n",
      "Epoch 138/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0153 - lr: 1.8781e-04\n",
      "\n",
      "Epoch 139: LearningRateScheduler setting learning rate to 0.00036562523393870056.\n",
      "Epoch 139/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 1.0121 - lr: 3.6563e-04\n",
      "\n",
      "Epoch 140: LearningRateScheduler setting learning rate to 0.0001878126169693503.\n",
      "Epoch 140/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 1.0214 - lr: 1.8781e-04\n",
      "\n",
      "Epoch 141: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 141/1000\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 1.0158 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 142: LearningRateScheduler setting learning rate to 0.00017403135527241525.\n",
      "Epoch 142/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0050 - lr: 1.7403e-04\n",
      "\n",
      "Epoch 143: LearningRateScheduler setting learning rate to 0.0003380627105448305.\n",
      "Epoch 143/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 1.0127 - lr: 3.3806e-04\n",
      "\n",
      "Epoch 144: LearningRateScheduler setting learning rate to 0.00017403135527241527.\n",
      "Epoch 144/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0162 - lr: 1.7403e-04\n",
      "\n",
      "Epoch 145: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 145/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 1.0079 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 146: LearningRateScheduler setting learning rate to 0.00016162821974517375.\n",
      "Epoch 146/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0071 - lr: 1.6163e-04\n",
      "\n",
      "Epoch 147: LearningRateScheduler setting learning rate to 0.00031325643949034753.\n",
      "Epoch 147/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 1.0135 - lr: 3.1326e-04\n",
      "\n",
      "Epoch 148: LearningRateScheduler setting learning rate to 0.00016162821974517378.\n",
      "Epoch 148/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0054 - lr: 1.6163e-04\n",
      "\n",
      "Epoch 149: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 149/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 1.0075 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 150: LearningRateScheduler setting learning rate to 0.00015046539777065638.\n",
      "Epoch 150/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0068 - lr: 1.5047e-04\n",
      "\n",
      "Epoch 151: LearningRateScheduler setting learning rate to 0.0002909307955413128.\n",
      "Epoch 151/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 1.0013 - lr: 2.9093e-04\n",
      "\n",
      "Epoch 152: LearningRateScheduler setting learning rate to 0.0001504653977706564.\n",
      "Epoch 152/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0072 - lr: 1.5047e-04\n",
      "\n",
      "Epoch 153: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 153/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 1.0035 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 154: LearningRateScheduler setting learning rate to 0.00014041885799359076.\n",
      "Epoch 154/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9920 - lr: 1.4042e-04\n",
      "\n",
      "Epoch 155: LearningRateScheduler setting learning rate to 0.00027083771598718155.\n",
      "Epoch 155/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 1.0066 - lr: 2.7084e-04\n",
      "\n",
      "Epoch 156: LearningRateScheduler setting learning rate to 0.0001404188579935908.\n",
      "Epoch 156/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0072 - lr: 1.4042e-04\n",
      "\n",
      "Epoch 157: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 157/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9905 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 158: LearningRateScheduler setting learning rate to 0.00013137697219423172.\n",
      "Epoch 158/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9988 - lr: 1.3138e-04\n",
      "\n",
      "Epoch 159: LearningRateScheduler setting learning rate to 0.0002527539443884634.\n",
      "Epoch 159/1000\n",
      "600/600 [==============================] - 14s 23ms/step - loss: 1.0056 - lr: 2.5275e-04\n",
      "\n",
      "Epoch 160: LearningRateScheduler setting learning rate to 0.0001313769721942317.\n",
      "Epoch 160/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9965 - lr: 1.3138e-04\n",
      "\n",
      "Epoch 161: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 161/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9989 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 162: LearningRateScheduler setting learning rate to 0.00012323927497480855.\n",
      "Epoch 162/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9978 - lr: 1.2324e-04\n",
      "\n",
      "Epoch 163: LearningRateScheduler setting learning rate to 0.0002364785499496171.\n",
      "Epoch 163/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9912 - lr: 2.3648e-04\n",
      "\n",
      "Epoch 164: LearningRateScheduler setting learning rate to 0.00012323927497480855.\n",
      "Epoch 164/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 1.0004 - lr: 1.2324e-04\n",
      "\n",
      "Epoch 165: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 165/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9994 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 166: LearningRateScheduler setting learning rate to 0.0001159153474773277.\n",
      "Epoch 166/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9825 - lr: 1.1592e-04\n",
      "\n",
      "Epoch 167: LearningRateScheduler setting learning rate to 0.0002218306949546554.\n",
      "Epoch 167/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 1.0005 - lr: 2.2183e-04\n",
      "\n",
      "Epoch 168: LearningRateScheduler setting learning rate to 0.0001159153474773277.\n",
      "Epoch 168/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9963 - lr: 1.1592e-04\n",
      "\n",
      "Epoch 169: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 169/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9862 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 170: LearningRateScheduler setting learning rate to 0.00010932381272959494.\n",
      "Epoch 170/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9911 - lr: 1.0932e-04\n",
      "\n",
      "Epoch 171: LearningRateScheduler setting learning rate to 0.00020864762545918988.\n",
      "Epoch 171/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9987 - lr: 2.0865e-04\n",
      "\n",
      "Epoch 172: LearningRateScheduler setting learning rate to 0.00010932381272959494.\n",
      "Epoch 172/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9862 - lr: 1.0932e-04\n",
      "\n",
      "Epoch 173: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 173/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9909 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 174: LearningRateScheduler setting learning rate to 0.00010339143145663545.\n",
      "Epoch 174/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9912 - lr: 1.0339e-04\n",
      "\n",
      "Epoch 175: LearningRateScheduler setting learning rate to 0.0001967828629132709.\n",
      "Epoch 175/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9831 - lr: 1.9678e-04\n",
      "\n",
      "Epoch 176: LearningRateScheduler setting learning rate to 0.00010339143145663545.\n",
      "Epoch 176/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9898 - lr: 1.0339e-04\n",
      "\n",
      "Epoch 177: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 177/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9904 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 178: LearningRateScheduler setting learning rate to 9.80522883109719e-05.\n",
      "Epoch 178/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9801 - lr: 9.8052e-05\n",
      "\n",
      "Epoch 179: LearningRateScheduler setting learning rate to 0.0001861045766219438.\n",
      "Epoch 179/1000\n",
      "600/600 [==============================] - 14s 23ms/step - loss: 0.9922 - lr: 1.8610e-04\n",
      "\n",
      "Epoch 180: LearningRateScheduler setting learning rate to 9.80522883109719e-05.\n",
      "Epoch 180/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9935 - lr: 9.8052e-05\n",
      "\n",
      "Epoch 181: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 181/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9844 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 182: LearningRateScheduler setting learning rate to 9.324705947987472e-05.\n",
      "Epoch 182/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9869 - lr: 9.3247e-05\n",
      "\n",
      "Epoch 183: LearningRateScheduler setting learning rate to 0.00017649411895974943.\n",
      "Epoch 183/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9865 - lr: 1.7649e-04\n",
      "\n",
      "Epoch 184: LearningRateScheduler setting learning rate to 9.324705947987472e-05.\n",
      "Epoch 184/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9878 - lr: 9.3247e-05\n",
      "\n",
      "Epoch 185: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 185/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9836 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 186: LearningRateScheduler setting learning rate to 8.892235353188725e-05.\n",
      "Epoch 186/1000\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 0.9841 - lr: 8.8922e-05\n",
      "\n",
      "Epoch 187: LearningRateScheduler setting learning rate to 0.0001678447070637745.\n",
      "Epoch 187/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9807 - lr: 1.6784e-04\n",
      "\n",
      "Epoch 188: LearningRateScheduler setting learning rate to 8.892235353188725e-05.\n",
      "Epoch 188/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9872 - lr: 8.8922e-05\n",
      "\n",
      "Epoch 189: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 189/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9856 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 190: LearningRateScheduler setting learning rate to 8.503011817869852e-05.\n",
      "Epoch 190/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9754 - lr: 8.5030e-05\n",
      "\n",
      "Epoch 191: LearningRateScheduler setting learning rate to 0.00016006023635739704.\n",
      "Epoch 191/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9863 - lr: 1.6006e-04\n",
      "\n",
      "Epoch 192: LearningRateScheduler setting learning rate to 8.503011817869852e-05.\n",
      "Epoch 192/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9860 - lr: 8.5030e-05\n",
      "\n",
      "Epoch 193: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 193/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9762 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 194: LearningRateScheduler setting learning rate to 8.152710636082867e-05.\n",
      "Epoch 194/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9815 - lr: 8.1527e-05\n",
      "\n",
      "Epoch 195: LearningRateScheduler setting learning rate to 0.00015305421272165733.\n",
      "Epoch 195/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9838 - lr: 1.5305e-04\n",
      "\n",
      "Epoch 196: LearningRateScheduler setting learning rate to 8.152710636082867e-05.\n",
      "Epoch 196/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9775 - lr: 8.1527e-05\n",
      "\n",
      "Epoch 197: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 197/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9827 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 198: LearningRateScheduler setting learning rate to 7.83743957247458e-05.\n",
      "Epoch 198/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9829 - lr: 7.8374e-05\n",
      "\n",
      "Epoch 199: LearningRateScheduler setting learning rate to 0.0001467487914494916.\n",
      "Epoch 199/1000\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 0.9760 - lr: 1.4675e-04\n",
      "\n",
      "Epoch 200: LearningRateScheduler setting learning rate to 7.83743957247458e-05.\n",
      "Epoch 200/1000\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 0.9832 - lr: 7.8374e-05\n",
      "\n",
      "Epoch 201: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 201/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9823 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 202: LearningRateScheduler setting learning rate to 7.553695615227121e-05.\n",
      "Epoch 202/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9703 - lr: 7.5537e-05\n",
      "\n",
      "Epoch 203: LearningRateScheduler setting learning rate to 0.00014107391230454243.\n",
      "Epoch 203/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9843 - lr: 1.4107e-04\n",
      "\n",
      "Epoch 204: LearningRateScheduler setting learning rate to 7.553695615227121e-05.\n",
      "Epoch 204/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9856 - lr: 7.5537e-05\n",
      "\n",
      "Epoch 205: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 205/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9745 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 206: LearningRateScheduler setting learning rate to 7.298326053704409e-05.\n",
      "Epoch 206/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9771 - lr: 7.2983e-05\n",
      "\n",
      "Epoch 207: LearningRateScheduler setting learning rate to 0.00013596652107408818.\n",
      "Epoch 207/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9828 - lr: 1.3597e-04\n",
      "\n",
      "Epoch 208: LearningRateScheduler setting learning rate to 7.298326053704409e-05.\n",
      "Epoch 208/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9741 - lr: 7.2983e-05\n",
      "\n",
      "Epoch 209: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 209/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9788 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 210: LearningRateScheduler setting learning rate to 7.068493448333968e-05.\n",
      "Epoch 210/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9753 - lr: 7.0685e-05\n",
      "\n",
      "Epoch 211: LearningRateScheduler setting learning rate to 0.00013136986896667937.\n",
      "Epoch 211/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9723 - lr: 1.3137e-04\n",
      "\n",
      "Epoch 212: LearningRateScheduler setting learning rate to 7.068493448333968e-05.\n",
      "Epoch 212/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9790 - lr: 7.0685e-05\n",
      "\n",
      "Epoch 213: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 213/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9791 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 214: LearningRateScheduler setting learning rate to 6.861644103500571e-05.\n",
      "Epoch 214/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9690 - lr: 6.8616e-05\n",
      "\n",
      "Epoch 215: LearningRateScheduler setting learning rate to 0.00012723288207001142.\n",
      "Epoch 215/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9776 - lr: 1.2723e-04\n",
      "\n",
      "Epoch 216: LearningRateScheduler setting learning rate to 6.861644103500571e-05.\n",
      "Epoch 216/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9836 - lr: 6.8616e-05\n",
      "\n",
      "Epoch 217: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 217/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9718 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 218: LearningRateScheduler setting learning rate to 6.675479693150514e-05.\n",
      "Epoch 218/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9728 - lr: 6.6755e-05\n",
      "\n",
      "Epoch 219: LearningRateScheduler setting learning rate to 0.00012350959386301028.\n",
      "Epoch 219/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9793 - lr: 1.2351e-04\n",
      "\n",
      "Epoch 220: LearningRateScheduler setting learning rate to 6.675479693150514e-05.\n",
      "Epoch 220/1000\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 0.9684 - lr: 6.6755e-05\n",
      "\n",
      "Epoch 221: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 221/1000\n",
      "600/600 [==============================] - 14s 23ms/step - loss: 0.9739 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 222: LearningRateScheduler setting learning rate to 6.507931723835464e-05.\n",
      "Epoch 222/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9771 - lr: 6.5079e-05\n",
      "\n",
      "Epoch 223: LearningRateScheduler setting learning rate to 0.00012015863447670926.\n",
      "Epoch 223/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9686 - lr: 1.2016e-04\n",
      "\n",
      "Epoch 224: LearningRateScheduler setting learning rate to 6.507931723835462e-05.\n",
      "Epoch 224/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9788 - lr: 6.5079e-05\n",
      "\n",
      "Epoch 225: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 225/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9767 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 226: LearningRateScheduler setting learning rate to 6.357138551451917e-05.\n",
      "Epoch 226/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9636 - lr: 6.3571e-05\n",
      "\n",
      "Epoch 227: LearningRateScheduler setting learning rate to 0.00011714277102903834.\n",
      "Epoch 227/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9748 - lr: 1.1714e-04\n",
      "\n",
      "Epoch 228: LearningRateScheduler setting learning rate to 6.357138551451917e-05.\n",
      "Epoch 228/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9797 - lr: 6.3571e-05\n",
      "\n",
      "Epoch 229: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 229/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9686 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 230: LearningRateScheduler setting learning rate to 6.221424696306726e-05.\n",
      "Epoch 230/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9751 - lr: 6.2214e-05\n",
      "\n",
      "Epoch 231: LearningRateScheduler setting learning rate to 0.00011442849392613451.\n",
      "Epoch 231/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9762 - lr: 1.1443e-04\n",
      "\n",
      "Epoch 232: LearningRateScheduler setting learning rate to 6.221424696306726e-05.\n",
      "Epoch 232/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9703 - lr: 6.2214e-05\n",
      "\n",
      "Epoch 233: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 233/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9729 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 234: LearningRateScheduler setting learning rate to 6.0992822266760535e-05.\n",
      "Epoch 234/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9735 - lr: 6.0993e-05\n",
      "\n",
      "Epoch 235: LearningRateScheduler setting learning rate to 0.00011198564453352107.\n",
      "Epoch 235/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9672 - lr: 1.1199e-04\n",
      "\n",
      "Epoch 236: LearningRateScheduler setting learning rate to 6.0992822266760535e-05.\n",
      "Epoch 236/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9745 - lr: 6.0993e-05\n",
      "\n",
      "Epoch 237: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 237/1000\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 0.9721 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 238: LearningRateScheduler setting learning rate to 5.989354004008448e-05.\n",
      "Epoch 238/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9650 - lr: 5.9894e-05\n",
      "\n",
      "Epoch 239: LearningRateScheduler setting learning rate to 0.00010978708008016896.\n",
      "Epoch 239/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9696 - lr: 1.0979e-04\n",
      "\n",
      "Epoch 240: LearningRateScheduler setting learning rate to 5.989354004008448e-05.\n",
      "Epoch 240/1000\n",
      "600/600 [==============================] - 14s 24ms/step - loss: 0.9786 - lr: 5.9894e-05\n",
      "\n",
      "Epoch 241: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 241/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9633 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 242: LearningRateScheduler setting learning rate to 5.8904186036076034e-05.\n",
      "Epoch 242/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9668 - lr: 5.8904e-05\n",
      "\n",
      "Epoch 243: LearningRateScheduler setting learning rate to 0.00010780837207215207.\n",
      "Epoch 243/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9729 - lr: 1.0781e-04\n",
      "\n",
      "Epoch 244: LearningRateScheduler setting learning rate to 5.8904186036076034e-05.\n",
      "Epoch 244/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9657 - lr: 5.8904e-05\n",
      "\n",
      "Epoch 245: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 245/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9724 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 246: LearningRateScheduler setting learning rate to 5.801376743246843e-05.\n",
      "Epoch 246/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9683 - lr: 5.8014e-05\n",
      "\n",
      "Epoch 247: LearningRateScheduler setting learning rate to 0.00010602753486493687.\n",
      "Epoch 247/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9650 - lr: 1.0603e-04\n",
      "\n",
      "Epoch 248: LearningRateScheduler setting learning rate to 5.801376743246843e-05.\n",
      "Epoch 248/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9701 - lr: 5.8014e-05\n",
      "\n",
      "Epoch 249: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 249/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9701 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 250: LearningRateScheduler setting learning rate to 5.721239068922159e-05.\n",
      "Epoch 250/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9615 - lr: 5.7212e-05\n",
      "\n",
      "Epoch 251: LearningRateScheduler setting learning rate to 0.00010442478137844318.\n",
      "Epoch 251/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9737 - lr: 1.0442e-04\n",
      "\n",
      "Epoch 252: LearningRateScheduler setting learning rate to 5.721239068922159e-05.\n",
      "Epoch 252/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9756 - lr: 5.7212e-05\n",
      "\n",
      "Epoch 253: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 253/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9623 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 254: LearningRateScheduler setting learning rate to 5.649115162029943e-05.\n",
      "Epoch 254/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9688 - lr: 5.6491e-05\n",
      "\n",
      "Epoch 255: LearningRateScheduler setting learning rate to 0.00010298230324059886.\n",
      "Epoch 255/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9712 - lr: 1.0298e-04\n",
      "\n",
      "Epoch 256: LearningRateScheduler setting learning rate to 5.649115162029943e-05.\n",
      "Epoch 256/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9639 - lr: 5.6491e-05\n",
      "\n",
      "Epoch 257: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 257/1000\n",
      "600/600 [==============================] - 14s 23ms/step - loss: 0.9687 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 258: LearningRateScheduler setting learning rate to 5.584203645826949e-05.\n",
      "Epoch 258/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9681 - lr: 5.5842e-05\n",
      "\n",
      "Epoch 259: LearningRateScheduler setting learning rate to 0.00010168407291653897.\n",
      "Epoch 259/1000\n",
      "600/600 [==============================] - 15s 25ms/step - loss: 0.9627 - lr: 1.0168e-04\n",
      "\n",
      "Epoch 260: LearningRateScheduler setting learning rate to 5.584203645826949e-05.\n",
      "Epoch 260/1000\n",
      "600/600 [==============================] - 16s 26ms/step - loss: 0.9706 - lr: 5.5842e-05\n",
      "\n",
      "Epoch 261: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 261/1000\n",
      "600/600 [==============================] - 14s 24ms/step - loss: 0.9714 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 262: LearningRateScheduler setting learning rate to 5.525783281244254e-05.\n",
      "Epoch 262/1000\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 0.9574 - lr: 5.5258e-05\n",
      "\n",
      "Epoch 263: LearningRateScheduler setting learning rate to 0.00010051566562488508.\n",
      "Epoch 263/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9675 - lr: 1.0052e-04\n",
      "\n",
      "Epoch 264: LearningRateScheduler setting learning rate to 5.525783281244254e-05.\n",
      "Epoch 264/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9710 - lr: 5.5258e-05\n",
      "\n",
      "Epoch 265: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 265/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9611 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 266: LearningRateScheduler setting learning rate to 5.4732049531198286e-05.\n",
      "Epoch 266/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9634 - lr: 5.4732e-05\n",
      "\n",
      "Epoch 267: LearningRateScheduler setting learning rate to 9.946409906239657e-05.\n",
      "Epoch 267/1000\n",
      "600/600 [==============================] - 15s 25ms/step - loss: 0.9685 - lr: 9.9464e-05\n",
      "\n",
      "Epoch 268: LearningRateScheduler setting learning rate to 5.4732049531198286e-05.\n",
      "Epoch 268/1000\n",
      "600/600 [==============================] - 15s 25ms/step - loss: 0.9616 - lr: 5.4732e-05\n",
      "\n",
      "Epoch 269: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 269/1000\n",
      "600/600 [==============================] - 14s 24ms/step - loss: 0.9673 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 270: LearningRateScheduler setting learning rate to 5.4258844578078455e-05.\n",
      "Epoch 270/1000\n",
      "600/600 [==============================] - 14s 23ms/step - loss: 0.9690 - lr: 5.4259e-05\n",
      "\n",
      "Epoch 271: LearningRateScheduler setting learning rate to 9.851768915615691e-05.\n",
      "Epoch 271/1000\n",
      "600/600 [==============================] - 14s 23ms/step - loss: 0.9596 - lr: 9.8518e-05\n",
      "\n",
      "Epoch 272: LearningRateScheduler setting learning rate to 5.4258844578078455e-05.\n",
      "Epoch 272/1000\n",
      "600/600 [==============================] - 16s 26ms/step - loss: 0.9682 - lr: 5.4259e-05\n",
      "\n",
      "Epoch 273: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 273/1000\n",
      "600/600 [==============================] - 16s 27ms/step - loss: 0.9661 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 274: LearningRateScheduler setting learning rate to 5.383296012027061e-05.\n",
      "Epoch 274/1000\n",
      "600/600 [==============================] - 14s 23ms/step - loss: 0.9554 - lr: 5.3833e-05\n",
      "\n",
      "Epoch 275: LearningRateScheduler setting learning rate to 9.766592024054122e-05.\n",
      "Epoch 275/1000\n",
      "600/600 [==============================] - 14s 24ms/step - loss: 0.9626 - lr: 9.7666e-05\n",
      "\n",
      "Epoch 276: LearningRateScheduler setting learning rate to 5.383296012027061e-05.\n",
      "Epoch 276/1000\n",
      "600/600 [==============================] - 14s 24ms/step - loss: 0.9698 - lr: 5.3833e-05\n",
      "\n",
      "Epoch 277: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 277/1000\n",
      "600/600 [==============================] - 14s 23ms/step - loss: 0.9602 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 278: LearningRateScheduler setting learning rate to 5.344966410824355e-05.\n",
      "Epoch 278/1000\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 0.9626 - lr: 5.3450e-05\n",
      "\n",
      "Epoch 279: LearningRateScheduler setting learning rate to 9.68993282164871e-05.\n",
      "Epoch 279/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9689 - lr: 9.6899e-05\n",
      "\n",
      "Epoch 280: LearningRateScheduler setting learning rate to 5.344966410824355e-05.\n",
      "Epoch 280/1000\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 0.9586 - lr: 5.3450e-05\n",
      "\n",
      "Epoch 281: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 281/1000\n",
      "600/600 [==============================] - 14s 24ms/step - loss: 0.9675 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 282: LearningRateScheduler setting learning rate to 5.31046976974192e-05.\n",
      "Epoch 282/1000\n",
      "600/600 [==============================] - 14s 24ms/step - loss: 0.9630 - lr: 5.3105e-05\n",
      "\n",
      "Epoch 283: LearningRateScheduler setting learning rate to 9.62093953948384e-05.\n",
      "Epoch 283/1000\n",
      "600/600 [==============================] - 15s 24ms/step - loss: 0.9570 - lr: 9.6209e-05\n",
      "\n",
      "Epoch 284: LearningRateScheduler setting learning rate to 5.31046976974192e-05.\n",
      "Epoch 284/1000\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 0.9658 - lr: 5.3105e-05\n",
      "\n",
      "Epoch 285: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 285/1000\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 0.9656 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 286: LearningRateScheduler setting learning rate to 5.279422792767728e-05.\n",
      "Epoch 286/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9546 - lr: 5.2794e-05\n",
      "\n",
      "Epoch 287: LearningRateScheduler setting learning rate to 9.558845585535456e-05.\n",
      "Epoch 287/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9673 - lr: 9.5588e-05\n",
      "\n",
      "Epoch 288: LearningRateScheduler setting learning rate to 5.279422792767728e-05.\n",
      "Epoch 288/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9683 - lr: 5.2794e-05\n",
      "\n",
      "Epoch 289: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 289/1000\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 0.9556 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 290: LearningRateScheduler setting learning rate to 5.251480513490955e-05.\n",
      "Epoch 290/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9660 - lr: 5.2515e-05\n",
      "\n",
      "Epoch 291: LearningRateScheduler setting learning rate to 9.50296102698191e-05.\n",
      "Epoch 291/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9660 - lr: 9.5030e-05\n",
      "\n",
      "Epoch 292: LearningRateScheduler setting learning rate to 5.251480513490955e-05.\n",
      "Epoch 292/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9557 - lr: 5.2515e-05\n",
      "\n",
      "Epoch 293: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 293/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9639 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 294: LearningRateScheduler setting learning rate to 5.226332462141859e-05.\n",
      "Epoch 294/1000\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 0.9659 - lr: 5.2263e-05\n",
      "\n",
      "Epoch 295: LearningRateScheduler setting learning rate to 9.452664924283718e-05.\n",
      "Epoch 295/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9590 - lr: 9.4527e-05\n",
      "\n",
      "Epoch 296: LearningRateScheduler setting learning rate to 5.226332462141859e-05.\n",
      "Epoch 296/1000\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 0.9637 - lr: 5.2263e-05\n",
      "\n",
      "Epoch 297: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 297/1000\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 0.9648 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 298: LearningRateScheduler setting learning rate to 5.2036992159276734e-05.\n",
      "Epoch 298/1000\n",
      "600/600 [==============================] - 14s 23ms/step - loss: 0.9522 - lr: 5.2037e-05\n",
      "\n",
      "Epoch 299: LearningRateScheduler setting learning rate to 9.407398431855347e-05.\n",
      "Epoch 299/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9625 - lr: 9.4074e-05\n",
      "\n",
      "Epoch 300: LearningRateScheduler setting learning rate to 5.2036992159276734e-05.\n",
      "Epoch 300/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9667 - lr: 5.2037e-05\n",
      "\n",
      "Epoch 301: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 301/1000\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 0.9567 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 302: LearningRateScheduler setting learning rate to 5.183329294334906e-05.\n",
      "Epoch 302/1000\n",
      "600/600 [==============================] - 14s 24ms/step - loss: 0.9600 - lr: 5.1833e-05\n",
      "\n",
      "Epoch 303: LearningRateScheduler setting learning rate to 9.366658588669812e-05.\n",
      "Epoch 303/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9661 - lr: 9.3667e-05\n",
      "\n",
      "Epoch 304: LearningRateScheduler setting learning rate to 5.183329294334906e-05.\n",
      "Epoch 304/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9572 - lr: 5.1833e-05\n",
      "\n",
      "Epoch 305: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 305/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9601 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 306: LearningRateScheduler setting learning rate to 5.1649963649014155e-05.\n",
      "Epoch 306/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9616 - lr: 5.1650e-05\n",
      "\n",
      "Epoch 307: LearningRateScheduler setting learning rate to 9.329992729802831e-05.\n",
      "Epoch 307/1000\n",
      "600/600 [==============================] - 12s 21ms/step - loss: 0.9565 - lr: 9.3300e-05\n",
      "\n",
      "Epoch 308: LearningRateScheduler setting learning rate to 5.1649963649014155e-05.\n",
      "Epoch 308/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9591 - lr: 5.1650e-05\n",
      "\n",
      "Epoch 309: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 309/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9623 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 310: LearningRateScheduler setting learning rate to 5.148496728411274e-05.\n",
      "Epoch 310/1000\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 0.9526 - lr: 5.1485e-05\n",
      "\n",
      "Epoch 311: LearningRateScheduler setting learning rate to 9.296993456822549e-05.\n",
      "Epoch 311/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9602 - lr: 9.2970e-05\n",
      "\n",
      "Epoch 312: LearningRateScheduler setting learning rate to 5.148496728411274e-05.\n",
      "Epoch 312/1000\n",
      "600/600 [==============================] - 13s 21ms/step - loss: 0.9647 - lr: 5.1485e-05\n",
      "\n",
      "Epoch 313: LearningRateScheduler setting learning rate to 1e-05.\n",
      "Epoch 313/1000\n",
      "600/600 [==============================] - 12s 20ms/step - loss: 0.9528 - lr: 1.0000e-05\n",
      "\n",
      "Epoch 314: LearningRateScheduler setting learning rate to 5.133647055570147e-05.\n",
      "Epoch 314/1000\n",
      "600/600 [==============================] - 13s 22ms/step - loss: 0.9603 - lr: 5.1336e-05\n",
      "Epoch 314: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = test_model.fit(x =X_train,epochs = 1000, steps_per_epoch = int(len(x_train)/batch_size),callbacks = [lr_callback, early_stopper])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x201ff8fed60>]"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history[\"loss\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d558789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test_model.predict([x_test.reshape(-1,784),x_test.reshape(-1,784),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "653f129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_labels = (results[0][:]+results[1][:]).argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ab2bd43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([5, 9, 5, ..., 6, 5, 9], dtype=int64)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05aec900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([7, 2, 1, ..., 4, 5, 6], dtype=uint8)"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a95f2580",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_unique_values = dict.fromkeys(np.unique(y_test),0)\n",
    "for i in range(len(y_test)):\n",
    "    nb_unique_values[y_test[i]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15beff60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{0: 980,\n 1: 1135,\n 2: 1032,\n 3: 1010,\n 4: 982,\n 5: 892,\n 6: 958,\n 7: 1028,\n 8: 974,\n 9: 1009}"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2833f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_nb_unique_values = dict.fromkeys(np.unique(y_predict_labels),0)\n",
    "for i in range(len(y_predict_labels)):\n",
    "    pred_nb_unique_values[y_predict_labels[i]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "014fe092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{0: 50, 3: 2640, 4: 332, 5: 2490, 6: 1083, 8: 1155, 9: 2250}"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_nb_unique_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "50f8b601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_ajustement(true, preds):\n",
    "    res_mat = np.zeros((len(np.unique(true)),len(np.unique(preds))))\n",
    "    for i in range(true.shape[0]):\n",
    "        res_mat[true[i],preds[i]] += 1\n",
    "    return res_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2dcf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "084e0dca",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 9 is out of bounds for axis 1 with size 7",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_22320/1493448104.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mres_mat\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mclass_ajustement\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_test\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0my_predict_labels\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_22320/474091542.py\u001B[0m in \u001B[0;36mclass_ajustement\u001B[1;34m(true, preds)\u001B[0m\n\u001B[0;32m      2\u001B[0m     \u001B[0mres_mat\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzeros\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munique\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrue\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0munique\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpreds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrue\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m         \u001B[0mres_mat\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mtrue\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mpreds\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m     \u001B[1;32mreturn\u001B[0m \u001B[0mres_mat\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mIndexError\u001B[0m: index 9 is out of bounds for axis 1 with size 7"
     ]
    }
   ],
   "source": [
    "res_mat = class_ajustement(y_test,y_predict_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426f396a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436899be",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(res_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21eb449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set()\n",
    "plt.figure(figsize=(16, 16))\n",
    "ax = sns.heatmap(res_mat/np.max(res_mat), vmin=0, vmax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8c8159",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "assignees = [False for i in range(nb_classes)]\n",
    "np.argsort(res_mat[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84439714",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_mat[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d53bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.flip(res_mat[:][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917600be",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(\"for class \",i,\" best match is \",np.flip(np.argsort(res_mat[:][i])), \"with confidence \",res_mat[:][i][np.flip(np.argsort(res_mat[:][i]))]/res_mat[i].sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
