{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6957de5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy as scp\n",
    "import sklearn as skl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec80008",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = ... # the number of repetition * batch_size must be <= to the dataset size\n",
    "nb_classes = ...\n",
    "\n",
    "def data_loader_custom(X,augmentation_1 = lambda x : x,augmentation_2 = lambda x : x, nb_classes = nb_classes):\n",
    "    \n",
    "    def configure_for_performance(ds):\n",
    "        ds = ds.shuffle(buffer_size=2*batch_size)\n",
    "        ds = ds.batch(batch_size=batch_size)\n",
    "        ds = ds.repeat()\n",
    "        ds = ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "        return ds\n",
    "    \n",
    "    \n",
    "    X_aug1 = augmentation_1(X)\n",
    "    \n",
    "    X_aug2 = augmentation_2(X)   \n",
    "    fake_labels = tf.data.Dataset.from_tensor_slices(np.zeros((len(X_aug1),nb_classes)))\n",
    "    X = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(X_aug1), tf.data.Dataset.from_tensor_slices(X_aug2)))\n",
    "    ds = tf.data.Dataset.zip(X,Y)   \n",
    "    ds = configure_for_performance(ds)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fc3b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss = tf.keras.metrics.Mean('loss', dtype=tf.float32)\n",
    "\n",
    "class custom_fit(tf.keras.Model):\n",
    "    \n",
    "    def tf_unif_prior_crossentropy(pred1,pred2):\n",
    "        N = batch_size\n",
    "        C = nb_classes\n",
    "        t_c = 0.1\n",
    "        t_r = 0.1\n",
    "        a = tf.nn.softmax(pred2/t_c, axis=0)\n",
    "        b = tf.linalg.norm(a, ord=1,axis=1)\n",
    "        c = a/tf.reshape(b,(N,-1))\n",
    "\n",
    "        part11 = c\n",
    "        d = tf.nn.softmax(pred1/t_r, axis=1)\n",
    "        part21 = tf.reshape(tf.math.log(N/C *d/tf.linalg.norm(d, ord=1, axis=0)), (-1,C))\n",
    "        loss_1 = -tf.math.reduce_sum( part11 * part21 , axis=-1)\n",
    "\n",
    "\n",
    "        a = tf.nn.softmax(pred1/t_c)\n",
    "        b = tf.linalg.norm(a, ord=1,axis=1)\n",
    "        c = a/tf.reshape(b,(N,-1))\n",
    "        part12 = c\n",
    "        d = tf.nn.softmax(pred2/t_r, axis=1)\n",
    "        part22 = tf.reshape(tf.math.log(N/C *d/tf.linalg.norm(d, ord=1, axis=0)),(-1,C))\n",
    "        loss_2 = -tf.math.reduce_sum( part12*part22 , axis=-1)\n",
    "\n",
    "        return tf.math.reduce_mean(1/2*(loss_1+loss_2))\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        input_data = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self(input_data, training=True) # forward pass \n",
    "            pred1 = outputs[0]\n",
    "            pred2 = outputs[1]\n",
    "            total_loss = self.tf_unif_prior_crossentropy(pred1,pred2)\n",
    "        gradients = tape.gradient(total_loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        avg_loss.update_state(total_loss)\n",
    "        return {\"loss\": avg_loss.result()}\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # We list our `Metric` objects here so that `reset_states()` can be\n",
    "        # called automatically at the start of each epoch\n",
    "        # or at the start of `evaluate()`.\n",
    "        # If you don't implement this property, you have to call\n",
    "        # `reset_states()` yourself at the time of your choosing.\n",
    "        return [avg_loss]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
